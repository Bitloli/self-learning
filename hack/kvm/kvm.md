# KVM

## ä½¿ç”¨ `kvm_stat` å¯ä»¥è§‚æµ‹æœ€æ ¸å¿ƒçš„å‡½æ•°

```txt
Event                                         Total %Total CurAvg/s
kvm_entry                                    337793   15.4    26107
kvm_exit                                     337787   15.4    26107
kvm_ack_irq                                  457205   20.9    25548
kvm_emulate_insn                             192824    8.8    16726
kvm_fast_mmio                                192514    8.8    16709
kvm_apic_accept_irq                          168566    7.7    15209
kvm_apicv_accept_irq                         168559    7.7    15209
kvm_msi_set_irq                              151693    6.9    13865
kvm_eoi                                       91441    4.2     5110
kvm_hv_timer_state                            22948    1.0     1818
kvm_msr                                       18642    0.9     1495
kvm_wait_lapic_expire                         14466    0.7     1166
kvm_pv_tlb_flush                               5097    0.2      384
kvm_pic_set_irq                                4832    0.2      369
kvm_set_irq                                    4788    0.2      369
kvm_ioapic_set_irq                             4788    0.2      369
kvm_fpu                                        3718    0.2      268
kvm_vcpu_wakeup                                3178    0.1      243
kvm_userspace_exit                             1860    0.1      134
kvm_pio                                        1600    0.1      119
kvm_hypercall                                  1188    0.1       83
kvm_mmio                                        484    0.0       27
vcpu_match_mmio                                 274    0.0       15
kvm_apic                                       1524    0.1        8
kvm_pvclock_update                               13    0.0        4
kvm_halt_poll_ns                                 42    0.0        3
Total                                       2187824          167463
```

## è¿‡ä¸€ä¸‹å®˜æ–¹æ–‡æ¡£
https://www.kernel.org/doc/html/latest/virt/kvm/index.html

## [ ] kvm ring
https://kvmforum2020.sched.com/event/eE4R/kvm-dirty-ring-a-new-approach-to-logging-peter-xu-red-hat

é¡ºä¾¿ç†è§£ä¸€ä¸‹:
```c
static const struct vm_operations_struct kvm_vcpu_vm_ops = {
	.fault = kvm_vcpu_fault,
};
```

## æ•´ç†å…³é”®çš„æ•°æ®ç»“æ„
- Each virtual CPU has an associated struct `kvm_run` data structure,
used to communicate information about the CPU between the kernel and user space.

## æ•´ç†ä¸€ä¸‹è·¯å¾„
- cpu hotplug

## TODO
1. VMPTRST å’Œ VMPTRLD
3. rsp_rdx
4. vmcs_config vmcs ä¸­é—´çš„å…·ä½“å†…å®¹æ˜¯ä»€ä¹ˆç”¨äºç®¡æ§ä»€ä¹ˆä¸œè¥¿
5. cpuid

MSR æ¥ check vmx çš„èƒ½åŠ›:
setup_vmcs_config çš„ä¸­é—´ï¼Œæ¥åˆ†æå…¶ä¸­çš„ä½œç”¨

Before system sftware can enter VMX operation, it enables VMX by setting CR4.VMXE[bit 13] = 1
`__vmx_enable`

æƒ³ä¸åˆ° : vmx_init_syscall åŠ¨æ€æ·»åŠ  syscall, å¯ä»¥åŠ¨æ€çš„ä¿®æ”¹ vcpu çš„å±æ€§.

vmcs çš„æ ¼å¼:
IA32_VMX_BASIC :

VPID åœ¨å†…æ ¸ä¸­çš„æ“ä½œæ–¹æ³• ?

## è®°å½•
[^2]: é…ç½®çš„ä»£ç éå¸¸è¯¦å°½
TODO : å†…æ ¸åˆ‡æ¢åˆ° long mode çš„æ–¹æ³•æ¯”è¿™é‡Œå¤æ‚å¤šäº†, çœ‹çœ‹[devos](https://wiki.osev.org/Setting_Up_Long_Moded)

The two modes are distinguished by the `dpl` (descriptor privilege level) field in segment register `cs.dpl=3`  in `cs` for user-mode, and zero for kernel-mode (not sure if this "level" equivalent to so-called ring3 and ring0).

In real mode kernelshould handle the segment registers carefully, while in x86-64, instructions syscall and sysret will properly set segment registers automatically, so we don't need to maintain segment registers manually.


This is just an example, we should *NOT* set user-accessible pages in hypervisor, user-accessible pages should be handled by our kernel.
> è¿™äº›ä¾‹å­ `mv->mem` çš„å†…å­˜æ˜¯ hypervisor çš„ï¼Œåˆ°åº•ä»€ä¹ˆæ˜¯ hypervisor ?

Registration of syscall handler can be achieved via setting special registers named `MSR (Model Specific Registers)`. We can get/set MSR in hypervisor through `ioctl` on `vcpufd`, or in kernel using instructions `rdmsr` and `wrmsr`.

> å…¶å®ä»£ç çš„æ‰€æœ‰çš„ç»†èŠ‚åº”è¯¥è¢«ä»”ç»†çš„ç†è§£æ¸…æ¥š TODO
> 1. ç»å…¸çš„ while(1) å¾ªç¯ï¼Œç„¶åå¤„ç†å„ç§æƒ…å†µçš„ç»“æ„åœ¨å“ªé‡Œ
> 2. ä¼¼ä¹ç›´æ¥ä»‹ç»äº†å†…æ ¸çš„è¿è¡Œæ–¹å¼è€Œå·²

## åˆ†æä¸€ä¸‹
https://www.owalle.com/2019/02/20/kvm-src-analysis

å¾ªç¯ä¾èµ– ?
x86.c : å­˜æ”¾æ•´ä¸ª x86 é€šç”¨çš„å‡½æ•°ï¼Œemulate.c å’Œ vmx.c ä¸­é—´éƒ½ä¼šä½¿ç”¨çš„ä»£ç 
vmx.c : å¤„ç†å„ç§ exit çš„æ“ä½œ, å…¶ä¸­å¯èƒ½ä¼šè°ƒç”¨ emulate.c çš„é‚£å¤„ç†
emulate.c : å„ç§æŒ‡ä»¤çš„æ¨¡æ‹Ÿ


## å…³é”®çš„æ•°æ®ç»“æ„
```c
struct kvm_x86_ops // éš¾é“æ˜¯ä¸ºäº†å°† kvm_get_msr å¯¹äºä¸åŒçš„ x86 æ¶æ„ä¸Š ?

struct x86_emulate_ops // å®šä¹‰çš„å‡½æ•°éƒ½æ˜¯ç»™ emulate.c ä½¿ç”¨

struct vcpu_vmx {
    struct kvm_vcpu       vcpu;
  ...
}

/*
 * x86 supports 4 paging modes (5-level 64-bit, 4-level 64-bit, 3-level 32-bit,
 * and 2-level 32-bit).  The kvm_mmu structure abstracts the details of the
 * current mmu mode.
 */
struct kvm_mmu {
```


## TODO

```c
  kvm_mmu_gva_to_gpa_read:5516
  kvm_mmu_gva_to_gpa_fetch:5523
  kvm_mmu_gva_to_gpa_write:5531
  kvm_mmu_gva_to_gpa_system:5540
```
- [ ] https://www.cnblogs.com/ck1020/p/6920765.html å…¶ä»–çš„æ–‡ç« 

- [ ] https://www.kernel.org/doc/ols/2007/ols2007v1-pages-225-230.pdf
    - çœ‹çœ‹ KVM çš„æ€»ä½“ä¸Šå±‚æ¶æ„æ€ä¹ˆå›äº‹
- [ ] x86.c :  mmio / pio çš„å¤„ç†
- [ ] emulate.c ä¸­é—´æ¨¡æ‹Ÿçš„æŒ‡ä»¤æ•°é‡æ˜¾ç„¶æ˜¯è¿œè¿œæ²¡æœ‰è¾¾åˆ°å®é™…ä¸ŠæŒ‡ä»¤æ•°é‡çš„ï¼Œè€Œä¸”éƒ½æ˜¯å„ç§åŸºæœ¬æŒ‡ä»¤çš„æ¨¡æ‹Ÿ
  - [ ] ä¸ºä»€ä¹ˆè¦è¿›è¡Œè¿™äº›æ¨¡æ‹Ÿ, vmx çš„å„ç§ handle å‡½æ•°ä¸ºä»€ä¹ˆåè€Œä¸èƒ½å¤„ç†è¿™äº›ç®€å•çš„æŒ‡ä»¤
  - [ ] å¾ˆå¤šæ“ä½œä¾èµ–äº vcs read / write ï¼Œä½†æ˜¯è¿™é‡Œä»…ä»…æ˜¯åˆ©ç”¨ `ctxt->ops` ç„¶åè¯» vcpu ä¸­çš„å†…å®¹
- [ ] vcpu çš„ regs å’Œ vmcs çš„ regs çš„å…³ç³»æ˜¯ä»€ä¹ˆ ?
- [ ] cpuid.c ä¸ºä»€ä¹ˆæœ‰ 1000 è¡Œ,  kvm_emulate_cpuid  å’Œ ioctl API
- [ ] è°ƒæŸ¥ä¸€ä¸‹ kvm_vcpu_gfn_to_hva
- [x] kvm çš„ host va çš„åœ°å€åœ¨å“ªé‡Œ ? åœ¨ä½¿ç”¨ kvm çš„çº¿ç¨‹çš„ç”¨æˆ·ç©ºé—´ä¸­
- [ ] mmu å’Œ flush å’Œ zap æœ‰ä»€ä¹ˆåŒºåˆ« ?
- [ ] ept å’Œ shadow page table æ„Ÿè§‰å¤„ç†æ–¹æ³•ç±»ä¼¼äº†: éƒ½æ˜¯ for_each_shadow_entryï¼Œkvm_mmu_get_page, link_shadow_page å’Œ mmu_set_spte
    - [ ] `FNAME(fetch)`
    - [ ] `__direct_map`

- [ ] å¯¹äº shadow page table, ä¸åŒçš„ process éƒ½æœ‰ä¸€å¥—ï¼Œä¸åŒ process çš„ cr3 çš„åŠ è½½æ˜¯ä»€ä¹ˆæ—¶å€™ ?
- [ ] åœ¨ FNAME(page_fault) çš„ä¸¤ä¸ªæ­¥éª¤åˆ¤æ–­ï¼Œå½“è§£å†³äº† guest page table çš„é—®é¢˜ä¹‹åï¼Œä¾æ—§å‘ç”Ÿ page fault, æ­¤æ—¶æ·»åŠ ä¸Šçš„ shadow page table æ˜¾ç„¶å¯ä»¥ track ä¸Š
- [ ] dirty log



## å‡½æ•°è°ƒç”¨è·¯å¾„

- kvm_arch_vcpu_ioctl_run
  - vcpu_run
    - vcpu_enter_guest
        - static_call(kvm_x86_vcpu_run)(vcpu)
          - svm_vcpu_run
            - svm_exit_handlers_fastpath
              - handle_fastpath_set_msr_irqoff
        - vmx_handle_exit
          - kvm_vmx_exit_handlers
            - `__kvm_get_msr`
              - `vmx_get_msr`

## x86.c overview
- VMCS çš„ IO
- timer pvclock tsc
- ioctl

- pio mmio å’Œ ä¸€èˆ¬çš„ IO çš„æ¨¡æ‹Ÿ
- emulate


1. debugfs
```c
static struct kmem_cache *x86_fpu_cache;
static struct kmem_cache *x86_emulator_cache;
```
2. kvm_on_user_return :
    1. user return ?
    2. share msr

3. exception_type

4. payload

æä¾›äº†å¾ˆå¤šå‡½æ•°è®¿é—®è®¾ç½® vcpuï¼Œæ¯”å¦‚ kvm_get_msr ä¹‹ç±»çš„
1. è°è°ƒç”¨ <- vmx.c å§ !
2. å®ç°çš„æ–¹æ³• : å°†å…¶æ”¾åœ¨ vmcs ä¸­ï¼Œ
ä» vmcs ä¸­é—´è¯»å– : å½“æƒ³è¦è®¿é—®çš„æ—¶å€™ï¼Œ

- [ ] vmcs æ˜¯å†…å­˜åŒºåŸŸï¼Œè¿˜ä¼šæ”¾åœ¨ CPU ä¸­é—´ï¼Œç”¨ æŒ‡ä»¤è¯»å†™çš„å†…å®¹

kvm_steal_time_set_preempted


## details

#### vmx_vcpu_run
vmx_exit_handlers_fastpath : é€šè¿‡ omit what æ¥ fast


#### kvm_read_guest_virt_helper
å†…æ ¸è¯»å– guest çš„å†…å­˜ï¼Œå› ä¸º guest çš„ä½¿ç”¨åœ°å€ç©ºé—´æ˜¯
ç”¨æˆ·æ€çš„ï¼Œæ‰€ä»¥
1. gva_to_gpa çš„åœ°å€åˆ‡æ¢
        gpa_t gpa = vcpu->arch.walk_mmu->gva_to_gpa(vcpu, addr, access,
2. kvm_vcpu_read_guest_page : copy_to_user è€Œå·²

## event injection
åœ¨ ./nested.md ä¸­çš„åŒå section ä¸­é—´

#### kvm_vcpu_flush_tlb_all

```c
static void kvm_vcpu_flush_tlb_all(struct kvm_vcpu *vcpu)
{
    ++vcpu->stat.tlb_flush;
    kvm_x86_ops.tlb_flush_all(vcpu);
}
```

## emulat.c
init_emulate_ctxt
x86_emulate_instruction :

```c
int kvm_emulate_instruction(struct kvm_vcpu *vcpu, int emulation_type)
{
    return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
}
```

1. emulate_ctxt çš„ä½¿ç”¨ä½ç½® :

    struct x86_emulate_ctxt *ctxt = vcpu->arch.emulate_ctxt;

- [x] emulate_ctxt.ops çš„è°ƒç”¨ä½ç½® ? åœ¨ emulate.c ä¸­é—´

1. ä¸ºä»€ä¹ˆä¼šå‡ºç° emulation_instruction çš„éœ€æ±‚ ?

```c
// å°† kvm_arch_vcpu_create è¢« kvm_vm_ioctl_create_vcpu å”¯ä¸€ call
int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
```

#### opcode_table çš„ä½¿ç”¨ä½ç½®

```c
static const struct opcode opcode_table[256] = {
```

æŒ‡ä»¤ç¼–ç :
```c
struct opcode {
    u64 flags : 56;
    u64 intercept : 8;
    union {
        int (*execute)(struct x86_emulate_ctxt *ctxt);
        const struct opcode *group;
        const struct group_dual *gdual;
        const struct gprefix *gprefix;
        const struct escape *esc;
        const struct instr_dual *idual;
        const struct mode_dual *mdual;
        void (*fastop)(struct fastop *fake);
    } u;
    int (*check_perm)(struct x86_emulate_ctxt *ctxt);
};
```

## intel processor tracing

- patch : https://lwn.net/Articles/741093/
- https://man7.org/linux/man-pages/man1/perf-intel-pt.1.html

#### `vmx_x86_ops`

- `struct x86_kvm_ops` : `vmx_x86_ops` ä¹Ÿæ˜¯å…¶ä¸­ä¸€ç§
- `x86_kvm_ops` : ä¸€ä¸ªç»å¸¸è®¿é—®çš„å˜é‡


```c
static struct kvm_x86_init_ops vmx_init_ops __initdata = {
    .cpu_has_kvm_support = cpu_has_kvm_support,
    .disabled_by_bios = vmx_disabled_by_bios,
    .check_processor_compatibility = vmx_check_processor_compat,
    .hardware_setup = hardware_setup,

    .runtime_ops = &vmx_x86_ops,
};

// åœ¨ KVM init çš„æ—¶å€™ï¼Œç¡®å®šä½¿ç”¨ä½•ç§ç¡¬ä»¶è®¾ç½®ï¼Œä½†æ˜¯ emulate è¿˜æ˜¯å­˜åœ¨çš„
int kvm_arch_hardware_setup(void *opaque)
{
  // ...
    memcpy(&kvm_x86_ops, ops->runtime_ops, sizeof(kvm_x86_ops));
  // ...
```

## emulate_ops å’Œ vmx_x86_ops çš„æ“ä½œå¯¹æ¯”
- vmx_x86_ops æä¾›äº†å„ç§æ“ä½œçš„ç¡¬ä»¶æ”¯æŒ.
- vmx çš„ kvm_vmx_exit_handlers éœ€è¦ emulate çš„ï¼Œä½†æ˜¯ emulator çš„å·¥ä½œéœ€è¦ä» emulator ä¸­é—´å¾—åˆ°æ•°æ®



## hyperv.c
æ¨¡æ‹Ÿ HyperV çš„å†…å®¹, ä½†æ˜¯ä¸ºä»€ä¹ˆéœ€è¦æ¨¡æ‹Ÿ HyperV ?

- kvm_hv_hypercall
- stimer

å®åœ¨æ˜¯æœ‰ç‚¹çœ‹ä¸æ‡‚:
https://docs.microsoft.com/en-us/virtualization/hyper-v-on-windows/reference/hyper-v-architecture

## irq.c
ä¼¼ä¹å¾ˆçŸ­ï¼Œä½†æ˜¯ lapic å¾ˆé•¿!


## ä¸­æ–­è™šæ‹ŸåŒ–
ä¸­æ–­è™šæ‹ŸåŒ–çš„å…³é”®åœ¨äºå¯¹ä¸­æ–­æ§åˆ¶å™¨çš„æ¨¡æ‹Ÿï¼Œæˆ‘ä»¬çŸ¥é“x86ä¸Šä¸­æ–­æ§åˆ¶å™¨ä¸»è¦æœ‰æ—§çš„ä¸­æ–­æ§åˆ¶å™¨PIC(intel 8259a)å’Œé€‚åº”äºSMPæ¡†æ¶çš„IOAPIC/LAPICä¸¤ç§ã€‚

https://luohao-brian.gitbooks.io/interrupt-virtualization/content/qemu-kvm-zhong-duan-xu-ni-hua-kuang-jia-fen-679028-4e2d29.html

æŸ¥è¯¢ GSI å·ä¸Šå¯¹åº”çš„æ‰€æœ‰çš„ä¸­æ–­å·:

ä» ioctl åˆ°ä¸‹å±‚ï¼Œkvm_vm_ioctl æ³¨å…¥çš„ä¸­æ–­ï¼Œæœ€åæ›´æ”¹äº† kvm_kipc_state:irr

kvm_kipc_state çš„ä¿¡æ¯å¦‚ä½•å‘ŠçŸ¥ CPU ? é€šè¿‡ kvm_pic_read_irq

## trace mmu

## mmu_spte_update
TODO : ä¸ºä»€ä¹ˆä¼šå­˜åœ¨ä¸€ä¸ª writable spte å’Œ read-only spte çš„åŒºåˆ† ?

```c
/* Rules for using mmu_spte_update:
 * Update the state bits, it means the mapped pfn is not changed.
 *
 * Whenever we overwrite a writable spte with a read-only one we
 * should flush remote TLBs. Otherwise rmap_write_protect
 * will find a read-only spte, even though the writable spte
 * might be cached on a CPU's TLB, the return value indicates this
 * case.
 *
 * Returns true if the TLB needs to be flushed
 */
static bool mmu_spte_update(u64 *sptep, u64 new_spte)
```

æ ¸å¿ƒå°±æ˜¯ WRITE_ONCE è€Œå·²ï¼Œä½†æ˜¯å­˜åœ¨å¾ˆå¤šæ£€æŸ¥

## ept

tdp_page_fault()->
gfn_to_pfn(); GPAåˆ°HPAçš„è½¬åŒ–åˆ†ä¸¤æ­¥å®Œæˆï¼Œåˆ†åˆ«é€šè¿‡gfn_to_hvaã€hva_to_pfnä¸¤ä¸ªå‡½æ•°å®Œæˆ
`__direct_map()`; å»ºç«‹EPTé¡µè¡¨ç»“æ„

ä¸ºä»€ä¹ˆ ept ä¹Ÿæ˜¯éœ€è¦å»ºç«‹ä¸€ä¸ª shadow page table ?


kvm_tdp_page_fault å’Œ ept_page_fault çš„å…³ç³»æ˜¯ä»€ä¹ˆ ?

## ept page table
- [ ] ept å’Œ shadow page table ä¸åº”è¯¥å…±äº«ç»“æ„å•Š

shadow page table : gva => hpa
ept : åº”è¯¥æ˜¯ GPA åˆ° HPA

- init_kvm_tdp_mmu
- kvm_mmu_alloc_page  : ç”³è¯· kvm_mmu_page ç©ºé—´ï¼Œè¯¥ç»“æ„è¡¨ç¤º EPT é¡µè¡¨é¡¹
- vmx_load_mmu_pgd : ä¼ å…¥çš„root_hpaä¹Ÿå°±ç›´æ¥å½“Guest CR3ç”¨ï¼Œå…¶å®å°±æ˜¯å½±å­é¡µè¡¨çš„åŸºå€ã€‚

- å½“CPUè®¿é—®EPTé¡µè¡¨æŸ¥æ‰¾HPAæ—¶ï¼Œå‘ç°ç›¸åº”çš„é¡µè¡¨é¡¹ä¸å­˜åœ¨ï¼Œåˆ™ä¼šå‘ç”ŸEPT Violationå¼‚å¸¸ï¼Œå¯¼è‡´VM-Exit

**GPAåˆ°HPAçš„æ˜ å°„å…³ç³»ç”±EPTé¡µè¡¨æ¥ç»´æŠ¤**

## ept å’Œ shadow page table ä¸­é—´çš„å†…å®¹
- ept å’Œ shadow page table çš„æ ¼å¼ç›¸åŒï¼Œè®©ç¡¬ä»¶è®¿é—®å¯ä»¥æ ¼å¼ç›¸åŒ
- ç»´æŠ¤ ept æ˜¯ä½¿ç”¨è½¯ä»¶çš„æ–¹æ³•ç»´æŠ¤çš„ï¼Œé‚£ä¹ˆ ept éƒ½æ˜¯ç‰©ç†åœ°å€

pgd : page global directory

kvm_mmu_load_pgd : `vcpu->arch.mmu->root_hpa` ä½œä¸ºå‚æ•°ä¼ é€’å‡ºå»

## mmu_alloc_root
è°ƒç”¨ kvm_mmu_get_pageï¼Œä½†æ˜¯å…¶åˆ©ç”¨ hash æ¥æŸ¥æ‰¾ï¼Œè¯´å¥½çš„ hash æ˜¯ç”¨äº id çš„å•Š

## arch.mmu->root_hpa å’Œ arch.mmu->root_pgd
- [x] æ˜¯ä¸æ˜¯ root_hpa è¢« direct ä½¿ç”¨ï¼Œroot_pgd è¢« shadow ä½¿ç”¨
  - å¹¶ä¸æ˜¯ï¼Œéƒ½ä¾èµ–äº hpa è¿›è¡Œ page walkï¼Œè€Œ root_pgd å°±æ˜¯ guest cr3 çš„å€¼ï¼Œè¿™æ˜¯ GPA


mmu_alloc_shadow_roots : `root_pgd = vcpu->arch.mmu->get_guest_pgd(vcpu);`
mmu_alloc_direct_roots : root_pgd = 0


get_guest_pgd çš„ä¸€èˆ¬æ³¨å†Œå‡½æ•°:
```c
static unsigned long get_cr3(struct kvm_vcpu *vcpu)
{
    return kvm_read_cr3(vcpu);
}

// è¯»å– cr3 ä¼¼ä¹ä¸æ˜¯ä¸€å®šä¼šä» vmcs ä¸­é—´è¯»å–
static inline ulong kvm_read_cr3(struct kvm_vcpu *vcpu)
{
    if (!kvm_register_is_available(vcpu, VCPU_EXREG_CR3))
        kvm_x86_ops.cache_reg(vcpu, VCPU_EXREG_CR3);
    return vcpu->arch.cr3;
}
```




1. `arch.mmu->root_hpa` çš„åˆå§‹åŒ–

mmu_alloc_direct_roots
```c
static int mmu_alloc_roots(struct kvm_vcpu *vcpu)
{
    if (vcpu->arch.mmu->direct_map)
        return mmu_alloc_direct_roots(vcpu);
    else
        return mmu_alloc_shadow_roots(vcpu);
}
```

## memory in kernel or qumu process
luohao's blog:

- [ ] rmap å­—æ®µçš„è§£é‡Šï¼Œé‚£ä¹ˆ memory æ˜¯ vmalloc åˆ†é…çš„ ?????
  - [ ] vmalloc çš„åˆ†é…æ˜¯ page fault çš„å— ?

```c
struct kvm_memory_slot {
    gfn_t base_gfn;                    // è¯¥å—ç‰©ç†å†…å­˜å—æ‰€åœ¨guest ç‰©ç†é¡µå¸§å·
    unsigned long npages;              //  è¯¥å—ç‰©ç†å†…å­˜å—å ç”¨çš„pageæ•°
    unsigned long flags;
    unsigned long *rmap;               // åˆ†é…è¯¥å—ç‰©ç†å†…å­˜å¯¹åº”çš„hostå†…æ ¸è™šæ‹Ÿåœ°å€ï¼ˆvmallocåˆ†é…ï¼‰
    unsigned long *dirty_bitmap;
    struct {
        unsigned long rmap_pde;
        int write_count;
    } *lpage_info[KVM_NR_PAGE_SIZES - 1];
    unsigned long userspace_addr;       // ç”¨æˆ·ç©ºé—´åœ°å€ï¼ˆQEMU)
    int user_alloc;
};
```


## parent_ptes
```c
static void kvm_mmu_mark_parents_unsync(struct kvm_mmu_page *sp)
{
    u64 *sptep;
    struct rmap_iterator iter;

    for_each_rmap_spte(&sp->parent_ptes, &iter, sptep) {
        mark_unsync(sptep);
    }
}

static void mark_unsync(u64 *spte)
{
    struct kvm_mmu_page *sp;
    unsigned int index;

    sp = sptep_to_sp(spte);
    index = spte - sp->spt;
    if (__test_and_set_bit(index, sp->unsync_child_bitmap))
        return;
    if (sp->unsync_children++)
        return;
    kvm_mmu_mark_parents_unsync(sp);
}
```
é€’å½’å‘ä¸Šï¼Œå½“å‘ç°å­˜åœ¨æœ‰äºº æ²¡æœ‰ unsync çš„æ—¶å€™ï¼Œåœ¨ unsync_child_bitmap ä¸­é—´è®¾ç½®æ ‡å¿—ä½ï¼Œ
å¹¶ä¸”å‘ä¸Šä¼ å¯¼ï¼Œç›´åˆ°å‘ç°æ²¡äººæ£€æµ‹è¿‡

link_shadow_page : mark_unsync çš„å”¯ä¸€è°ƒç”¨ä½ç½®
kvm_unsync_page : kvm_mmu_mark_parents_unsync å”¯ä¸€è°ƒç”¨ä½ç½®

mmu_need_write_protect : å¯¹äºsp

#### mmu_need_write_protect
for_each_gfn_indirect_valid_sp : ä¸€ä¸ª gfn å¯ä»¥
åŒæ—¶å¯¹åº”å¤šä¸ª shadow pageï¼ŒåŸå› æ˜¯ä¸€ä¸ª guest page å¯ä»¥å¯¹åº”å¤šä¸ª shadow page


> hash : å®ç° guest page tabel å’Œ shadow page çš„æ˜ å°„

> rmap_add å¤„ç†çš„æ˜¯ :  gfn å’Œå…¶å¯¹åº”çš„ pte çš„å¯¹åº”å…³ç³»


## role.quadrant
ä½œç”¨: ä¸€ä¸ª guest åœ°å€å¯¹åº”çš„ page table

get_written_sptes : ä¾é  gpa çš„ page_offset è®¡ç®—å‡ºæ¥ï¼Œç„¶åå’Œ `sp->role.quadrant` å¯¹æ¯”

#### obsolete sp

```c
static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
{
    return sp->role.invalid ||
           unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
}
```

#### gfn_to_rmap
RMAP_RECYCLE_THRESHOLD å±…ç„¶æ˜¯ 1000

## gfn_track

```diff
 History:        #0
 Commit:         3d0c27ad6ee465f174b09ee99fcaf189c57d567a
 Author:         Xiao Guangrong <guangrong.xiao@linux.intel.com>
 Committer:      Paolo Bonzini <pbonzini@redhat.com>
 Author Date:    Wed 24 Feb 2016 09:51:11 AM UTC
 Committer Date: Thu 03 Mar 2016 01:36:21 PM UTC

 KVM: MMU: let page fault handler be aware tracked page

 The page fault caused by write access on the write tracked page can not
 be fixed, it always need to be emulated. page_fault_handle_page_track()
 is the fast path we introduce here to skip holding mmu-lock and shadow
 page table walking

 However, if the page table is not present, it is worth making the page
 table entry present and readonly to make the read access happy

 mmu_need_write_protect() need to be cooked to avoid page becoming writable
 when making page table present or sync/prefetch shadow page table entries

 Signed-off-by: Xiao Guangrong <guangrong.xiao@linux.intel.com>
 Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
```
-  [ ] tracked çš„ page ä¸èƒ½è¢« fixed, å¿…é¡»è¢«æ¨¡æ‹Ÿï¼Œä¸ºå•¥ ?

gfn_track å…¶å®æ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„ï¼Œå‘Šè¯‰è¯¥ é¡µé¢è¢« track äº†ï¼Œç„¶å
kvm_mmu_page_fault ä¸­é—´å°†ä¼šè°ƒç”¨ x86_emulate_instruction æ¥å¤„ç†ï¼Œ
ä¼¼ä¹ç„¶åé€šè¿‡ mmu_notifier ä½¿ç”¨ kvm_mmu_pte_write æ¥æ›´æ–° guest page table

#### page_fault_handle_page_track
direct_page_fault å’Œ FNAME(page_fault) è°ƒç”¨ï¼Œ
ä¼¼ä¹å¦‚æœè¢« trackï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªå‡½æ•°ä¼šè¿”å› RET_PF_EMULATE


## track æœºåˆ¶
track å’Œ dirty bitmap å®é™…ä¸Šæ˜¯ä¸¤ä¸ªäº‹æƒ…å§!

å¯¹äºåŠ ä»¥ç»´æŠ¤çš„:
kvm_slot_page_track_add_page :
kvm_slot_page_track_remove_page :
==> update_gfn_track

- [ ] ä¸¤ä¸ªå‡½æ•°ï¼Œè°ƒç”¨ update,  éƒ½æ˜¯å¯¹äº gfn_track çš„åŠ å‡ 1 è€Œå·²

åˆ†åˆ«è¢« account_shadowed å’Œ unaccount_shadowed è°ƒç”¨

`__kvm_mmu_prepare_zap_page` : è¢«å„ç§ zap page è°ƒç”¨ï¼Œå¹¶ä¸”é…åˆ commit_zap ä½¿ç”¨
=> unaccount_shadowed

kvm_mmu_get_page :
=> account_shadowed




1. kvm_mmu_page_write

```c
void kvm_mmu_init_vm(struct kvm *kvm)
{
    struct kvm_page_track_notifier_node *node = &kvm->arch.mmu_sp_tracker;

    node->track_write = kvm_mmu_pte_write;
    node->track_flush_slot = kvm_mmu_invalidate_zap_pages_in_memslot;
    kvm_page_track_register_notifier(kvm, node);
}
```
kvm_mmu_get_page: å½“ä¸æ˜¯ direct æ¨¡å¼ï¼Œé‚£ä¹ˆéœ€è¦å¯¹äº kvm_mmu_alloc_page çš„ page è¿›è¡Œ account_shadowed
=> account_shadowed :
=> kvm_slot_page_track_add_page

**æ‰€ä»¥ï¼Œä¿æŠ¤çš„æ˜¯ shadow page table ?**

```c
static void account_shadowed(struct kvm *kvm, struct kvm_mmu_page *sp)
{
    struct kvm_memslots *slots;
    struct kvm_memory_slot *slot;
    gfn_t gfn;

    kvm->arch.indirect_shadow_pages++;
    gfn = sp->gfn;
    slots = kvm_memslots_for_spte_role(kvm, sp->role);
    slot = __gfn_to_memslot(slots, gfn);

    /* the non-leaf shadow pages are keeping readonly. */
    if (sp->role.level > PG_LEVEL_4K)
        return kvm_slot_page_track_add_page(kvm, slot, gfn,
                            KVM_PAGE_TRACK_WRITE);

    kvm_mmu_gfn_disallow_lpage(slot, gfn);
}
```
- [ ] ä¸ºä»€ä¹ˆä¸ä¿æŠ¤ leaf shadow page ?

> TOBECON

## track mode

> - dirty tracking:
>    report writes to guest memory to enable live migration
>    and framebuffer-based displays

åŸæ¥ tracing æ˜¯ dirty çš„



```diff
 KVM: page track: add the framework of guest page tracking

 The array, gfn_track[mode][gfn], is introduced in memory slot for every
 guest page, this is the tracking count for the gust page on different
 modes. If the page is tracked then the count is increased, the page is
 not tracked after the count reaches zero

 We use 'unsigned short' as the tracking count which should be enough as
 shadow page table only can use 2^14 (2^3 for level, 2^1 for cr4_pae, 2^2
 for quadrant, 2^3 for access, 2^1 for nxe, 2^1 for cr0_wp, 2^1 for
 smep_andnot_wp, 2^1 for smap_andnot_wp, and 2^1 for smm) at most, there
 is enough room for other trackers

 Two callbacks, kvm_page_track_create_memslot() and
 kvm_page_track_free_memslot() are implemented in this patch, they are
 internally used to initialize and reclaim the memory of the array

 Currently, only write track mode is supported
```

#### gfn_to_memslot_dirty_bitmap
`slot->dirty_bitmap` éƒ½åœ¨ kvm_main ä¸Šé¢è®¿é—®

pte_prefetch_gfn_to_pfn


- [ ] dirty æŒ‡çš„æ˜¯ è° ç›¸å¯¹äº è° æ˜¯ dirty çš„

```c
/**
 * kvm_vm_ioctl_get_dirty_log - get and clear the log of dirty pages in a slot
 * @kvm: kvm instance
 * @log: slot id and address to which we copy the log
 *
 * Steps 1-4 below provide general overview of dirty page logging. See
 * kvm_get_dirty_log_protect() function description for additional details.
 *
 * We call kvm_get_dirty_log_protect() to handle steps 1-3, upon return we
 * always flush the TLB (step 4) even if previous step failed  and the dirty
 * bitmap may be corrupt. Regardless of previous outcome the KVM logging API
 * does not preclude user space subsequent dirty log read. Flushing TLB ensures
 * writes will be marked dirty for next log read.
 *
 *   1. Take a snapshot of the bit and clear it if needed.
 *   2. Write protect the corresponding page.
 *   3. Copy the snapshot to the userspace.
 *   4. Flush TLB's if needed.
 */
static int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm,
                      struct kvm_dirty_log *log)
{
    int r;

    mutex_lock(&kvm->slots_lock);

    r = kvm_get_dirty_log_protect(kvm, log);

    mutex_unlock(&kvm->slots_lock);
    return r;
}
```

https://terenceli.github.io/%E6%8A%80%E6%9C%AF/2018/08/11/dirty-pages-tracking-in-migration

> So here for every gfn, we remove the write access. After return from this ioctl, the guestâ€™s RAM has been marked no write access, every write to this will exit to KVM make the page dirty. This means â€˜start the dirty logâ€™.


- [ ] kvm_mmu_slot_apply_flags : å®é™…ä½œç”¨æ˜¯ dirty log

## kvm_sync_page
kvm_sync_pages : å¯¹äº gfn (å…¶å®æ˜¯ gva å…³è”çš„ vcpu) å…¨éƒ¨æ›´æ–°, é€šè¿‡è°ƒç”¨ kvm_sync_page

kvm_mmu_sync_roots : ä»æ ¹èŠ‚ç‚¹æ›´æ–°æ›´æ–° => (mmu_sync_children : å°†æ•´ä¸ª children è¿›è¡Œ sync)

æœ€ç»ˆè°ƒç”¨ sync_page å‡½æ•°æŒ‡é’ˆç»´æŒç”Ÿæ´»







## mmio
- [ ] å¯¹äº host è€Œè¨€ï¼Œå­˜åœ¨ pcie åˆ†é… mmio çš„åœ°å€ç©ºé—´ï¼Œåœ¨è™šæ‹Ÿæœºä¸­é—´ï¼Œè¿™ä¸€ä¸ªæ˜¯å¦‚ä½•åˆ†é…çš„ MMIO ç©ºé—´çš„

```c
static bool is_mmio_spte(u64 spte)
{
    return (spte & SPTE_SPECIAL_MASK) == SPTE_MMIO_MASK;
}
```

- generation åªæ˜¯ä¸ºäº† MMIO è€Œå¤„ç†çš„


> - if the RSV bit of the error code is set, the page fault is caused by guest
>  accessing MMIO and cached MMIO information is available.
>
>  - walk shadow page table
>  - check for valid generation number in the spte (see "Fast invalidation of
>    MMIO sptes" below)
>  - cache the information to `vcpu->arch.mmio_gva`, `vcpu->arch.mmio_access` and
>    `vcpu->arch.mmio_gfn`, and call the emulator


## mmio generation
ğŸ‘‡è®°å½• mmu.rst çš„å†…å®¹:
è™½ç„¶çš„ç¡®è§£é‡Šäº† mmio ä½¿ç”¨ generation çš„åŸå› ï¼Œä½†æ˜¯ä¸‹é¢çš„é—®é¢˜å€¼å¾—ç†è§£:
- [ ] As mentioned in "Reaction to events" above, kvm will cache MMIO information in leaf sptes.
  - [ ] å¦‚æœä¸ cache, è¿™äº›æ•°æ®æ”¾åœ¨é‚£é‡Œ

- [ ] When a new memslot is added or an existing memslot is changed, this information may become stale and needs to be invalidated.
  - [ ] ä¸ºä»€ä¹ˆ memslot å¢åŠ ï¼Œå¯¼è‡´æ•°æ®å¤±æ•ˆ

Unfortunately, a single memory access might access kvm_memslots(kvm) multiple
times, the last one happening when the generation number is retrieved and
stored into the MMIO spte.  Thus, the MMIO spte might be created based on
out-of-date information, but with an up-to-date generation number.

- [ ] To avoid this, the generation number is incremented again after synchronize_srcu
returns;

- [ ] æ‰¾åˆ°è®¿é—® pte æ¥æ¯”è¾ƒ generation, å‘ç° out of dateï¼Œç„¶å slow path çš„ä»£ç 

## TODO : shadow flood

#### kvm_vm_ioctl_set_memory_region

#### kvm_vcpu_unmap

#### kvm_read_guest
- [ ] ä¸ºä»€ä¹ˆè¦å¤„ç† guest page æœºåˆ¶

#### kvm_vcpu_fault
> é…åˆ vcpu ioctl
```c
static int create_vcpu_fd(struct kvm_vcpu *vcpu)
{
    char name[8 + 1 + ITOA_MAX_LEN + 1];

    snprintf(name, sizeof(name), "kvm-vcpu:%d", vcpu->vcpu_id);
    return anon_inode_getfd(name, &kvm_vcpu_fops, vcpu, O_RDWR | O_CLOEXEC);
}
```


## shadow page table çš„åå¤„
- Simplified VMM design. éœ€è¦å¤„ç† shadow page table å’Œä¸¤çº§ç¿»è¯‘çš„åŒæ­¥é—®é¢˜
- Guest page table modifications need not be trapped, hence VM exits reduced. åŒæ­¥
- Reduced memory footprint compared to shadow page table algorithms. shadow table ä¼šå ç”¨ç©ºé—´


## hypercall
https://stackoverflow.com/questions/33590843/implementing-a-custom-hypercall-in-kvm

x86.c: kvm_emulate_hypercall

```c
/* For KVM hypercalls, a three-byte sequence of either the vmcall or the vmmcall
 * instruction.  The hypervisor may replace it with something else but only the
 * instructions are guaranteed to be supported.
 *
 * Up to four arguments may be passed in rbx, rcx, rdx, and rsi respectively.
 * The hypercall number should be placed in rax and the return value will be
 * placed in rax.  No other registers will be clobbered unless explicitly
 * noted by the particular hypercall.
 */

static inline long kvm_hypercall0(unsigned int nr)
{
    long ret;
    asm volatile(KVM_HYPERCALL
             : "=a"(ret)
             : "a"(nr)
             : "memory");
    return ret;
}
```
host å‘é€ hypercall çš„ä¹‹åï¼Œé€ æˆä» host ä¸­é—´é€€å‡ºï¼Œç„¶å æœ€åè°ƒç”¨åˆ° kvm_emulate_hypercall, å®é™…ä¸Šæ”¯æŒçš„æ“ä½œå¾ˆå°‘

```c
int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
{
    unsigned long nr, a0, a1, a2, a3, ret;
    int op_64_bit;

    if (kvm_hv_hypercall_enabled(vcpu->kvm))
        return kvm_hv_hypercall(vcpu);

```

## manual notes
- Table C-1. Basic Exit Reasons
