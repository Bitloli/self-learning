## é—®é¢˜
1. ä¸ºä»€ä¹ˆ is_tdp_mmu çš„è®¡ç®—å¦‚æ­¤å¤æ‚ï¼Œä¸åº”è¯¥æ˜¯ä¸€ä¸ª macro æ¥å®ç°å—?

```c
static inline bool is_tdp_mmu(struct kvm_mmu *mmu)
{
	struct kvm_mmu_page *sp;
	hpa_t hpa = mmu->root.hpa;

	if (WARN_ON(!VALID_PAGE(hpa)))
		return false;

	/*
	 * A NULL shadow page is legal when shadowing a non-paging guest with
	 * PAE paging, as the MMU will be direct with root_hpa pointing at the
	 * pae_root page, not a shadow page.
	 */
	sp = to_shadow_page(hpa);
	return sp && is_tdp_mmu_page(sp) && sp->root_count;
}
```
2. paging_tmpl.h åªæ˜¯å’Œ page table ç›¸å…³å§

#### `__direct_map`
1. for_each_shadow_entry : å› ä¸ºå¤šä¸ª shadow page æ˜ å°„ä¸€ä¸ª page table

#### https://luohao-brian.gitbooks.io/interrupt-virtualization/content/kvmzhi-nei-cun-xu-ni531628-kvm-mmu-virtualization.html

è·å¾—ç¼ºé¡µå¼‚å¸¸å‘ç”Ÿæ—¶çš„CR2,åŠå½“æ—¶è®¿é—®çš„è™šæ‹Ÿåœ°å€ï¼›
è¿›å…¥
```
kvm_mmu_page_fault()(vmx.c)->
r = vcpu->arch.mmu.page_fault(vcpu, cr2, error_code);(mmu.c)->
FNAME(page_fault)(struct kvm_vcpu *vcpu, gva_t addr, u32 error_code)(paging_tmpl.h)->
FNAME(walk_addr)()
```
æŸ¥guesté¡µè¡¨ï¼Œç‰©ç†åœ°å€æ˜¯å¦å­˜åœ¨ï¼Œ è¿™æ—¶è‚¯å®šæ˜¯ä¸å­˜åœ¨çš„
The page is not mapped by the guest. Let the guest handle it.
`inject_page_fault()->kvm_inject_page_fault()` å¼‚å¸¸æ³¨å…¥æµç¨‹ï¼›

> åªè¦æ˜¯ mmu ä¸­é—´è®¿é—®å¤±è´¥éƒ½æ˜¯éœ€è¦è¿›è¡Œ vm exit çš„ï¼Œå¦‚æœå‘ç°æ˜¯ guest çš„é—®é¢˜ï¼Œé‚£ä¹ˆé€šçŸ¥ guest
> TODO æ‰¾åˆ°å¯¹äº guest çš„ page table è¿›è¡Œ walk çš„æ–¹æ³•
> Guest æå®šä¹‹åï¼Œé‚£ä¹ˆ
> TODO TLB çš„æŸ¥æ‰¾ä¸åˆ°ï¼Œè¢« VMM æˆªè·åº”è¯¥æ˜¯éœ€è¦ ç¡¬ä»¶æ”¯æŒçš„å§!

ä¸ºäº†å¿«é€Ÿæ£€ç´¢GUESTé¡µè¡¨æ‰€å¯¹åº”çš„çš„å½±å­é¡µè¡¨ï¼ŒKVM ä¸ºæ¯ä¸ªGUESTéƒ½ç»´æŠ¤äº†ä¸€ä¸ªå“ˆå¸Œ
è¡¨ï¼Œå½±å­é¡µè¡¨å’ŒGUESTé¡µè¡¨é€šè¿‡æ­¤å“ˆå¸Œè¡¨è¿›è¡Œæ˜ å°„ã€‚å¯¹äºæ¯ä¸€ä¸ªGUESTæ¥è¯´ï¼ŒGUEST
çš„é¡µç›®å½•å’Œé¡µè¡¨éƒ½æœ‰å”¯ä¸€çš„GUESTç‰©ç†åœ°å€ï¼Œé€šè¿‡é¡µç›®å½•/é¡µè¡¨çš„å®¢æˆ·æœºç‰©ç†åœ°å€å°±
å¯ä»¥åœ¨å“ˆå¸Œé“¾è¡¨ä¸­å¿«é€Ÿåœ°æ‰¾åˆ°å¯¹åº”çš„å½±å­é¡µç›®å½•/é¡µè¡¨ã€‚
> æ˜¾ç„¶ä¸å¯èƒ½ä½¿ç”¨ä¿å­˜æ‰€æœ‰çš„ç‰©ç†åœ°å€ï¼Œä»è™šæ‹Ÿæœºåªä¼šå°†è™šæ‹Ÿæœºä½¿ç”¨çš„ç‰©ç†åœ°å€å¤„ç†æ‰

> å¡«å……è¿‡ç¨‹

mmu_alloc_root =>
`__direct_map` => kvm_mmu_get_page =>


æ„Ÿè§‰è¿™é‡Œè¿˜æ˜¯ shadow çš„å¤„ç†æœºåˆ¶ï¼Œé‚£ä¹ˆ ept åœ¨å“ªé‡Œ ?
```c
static int __direct_map(struct kvm_vcpu *vcpu, gpa_t gpa, int write,
            int map_writable, int max_level, kvm_pfn_t pfn,
            bool prefault, bool account_disallowed_nx_lpage)
{
  // TODO æ˜¯åœ¨å¯¹äºè°è¿›è¡Œ walk ? åº”è¯¥ä¸æ˜¯æ˜¯å¯¹äº shadow page è¿›è¡Œçš„
  // shadow page ä¹Ÿæ˜¯åˆ’åˆ†ä¸º leaf å’Œ nonleaf çš„ï¼Œä¹Ÿå°±æ˜¯è¿™æ˜¯å¯¹äº shadow çš„
  //
  // shadow page å½¢æˆä¸€ä¸ªå±‚æ¬¡ç»“æ„çš„ç›®çš„æ˜¯ä»€ä¹ˆ ?
    struct kvm_shadow_walk_iterator it;
    struct kvm_mmu_page *sp;
    int level, ret;
    gfn_t gfn = gpa >> PAGE_SHIFT;
    gfn_t base_gfn = gfn;

    if (WARN_ON(!VALID_PAGE(vcpu->arch.mmu->root_hpa)))
        return RET_PF_RETRY;

  // TODO level generation çš„å«ä¹‰
  // level : éš¾é“ shadow page table ä¹Ÿæ˜¯éœ€è¦å¤šä¸ª level
    level = kvm_mmu_hugepage_adjust(vcpu, gfn, max_level, &pfn);

    for_each_shadow_entry(vcpu, gpa, it) {
        /*
         * We cannot overwrite existing page tables with an NX
         * large page, as the leaf could be executable.
         */
        disallowed_hugepage_adjust(it, gfn, &pfn, &level);

        base_gfn = gfn & ~(KVM_PAGES_PER_HPAGE(it.level) - 1);
        if (it.level == level)
            break;

        drop_large_spte(vcpu, it.sptep);
        if (!is_shadow_present_pte(*it.sptep)) {
            sp = kvm_mmu_get_page(vcpu, base_gfn, it.addr,
                          it.level - 1, true, ACC_ALL);

            link_shadow_page(vcpu, it.sptep, sp);
            if (account_disallowed_nx_lpage)
                account_huge_nx_page(vcpu->kvm, sp);
        }
    }

    ret = mmu_set_spte(vcpu, it.sptep, ACC_ALL,
               write, level, base_gfn, pfn, prefault,
               map_writable);
    direct_pte_prefetch(vcpu, it.sptep);
    ++vcpu->stat.pf_fixed;
    return ret;
}
```
==> kvm_mmu_get_page : åº”è¯¥ä¿®æ”¹ä¸º get_shadow_page
==> kvm_page_table_hashfn : åˆ©ç”¨ gfn ä½œä¸º hash å¿«é€Ÿå®šä½ shadow_page
==> kvm_mmu_alloc_page : åˆ†é…å¹¶ä¸”åˆå§‹åŒ–ä¸€ä¸ª shadow page table

æ³¨æ„ : shadow page table ä¼¼ä¹å¯ä»¥å­˜æ”¾ shadow page table entry çš„

**TODO** è°ƒæŸ¥ kvm_mmu_alloc_page çš„åˆ›å»ºçš„ kvm_mmu_page çš„ç®¡ç†å†…å®¹, ä¼¼ä¹ rule è¯´æ˜äº†å¾ˆå¤šä¸œè¥¿

The hypervisor computes the guest virtual to
host physical mapping on the fly and stores it in
a new set of page tables

https://www.linux-kvm.org/images/e/e5/KvmForum2007%24shadowy-depths-of-the-kvm-mmu.pdfhttps://www.linux-kvm.org/images/e/e5/KvmForum2007%24shadowy-depths-of-the-kvm-mmu.pdf

emmmm : ä¸€ä¸ªç‰©ç†é¡µé¢ï¼Œåœ¨ host çœ‹æ¥æ˜¯ç»™ host ä½¿ç”¨çš„ï¼Œwrite protect  å¯ä»¥åœ¨ guest ä¸­é—´ï¼Œ
ä¹Ÿæ˜¯å¯ä»¥æ”¾åœ¨ host ä¸­é—´ã€‚

emmmm : ä»€ä¹ˆæƒ…å†µä¸‹ï¼Œä¸€ä¸ª hva å¯ä»¥è¢«å¤šä¸ª gpa æ˜ å°„ ?

å¯¹äº guest çš„é‚£äº› page tableï¼Œéœ€è¦é€šè¿‡ `page->private` å…³è”èµ·æ¥.

- When we shadow a guest page, we iterate over
the reverse map and remove write access

- When adding write permission to a page, we
check whether the page has a shadow

- **We can have multiple shadow pages for a
single guest page â€“ one for each role**

#### shadow page descriptor
TODO : shadow page table åœ¨ TLB miss çš„æ—¶å€™ï¼Œè§¦å‘ exception å— ?

- [x] æ—¢ç„¶ hash table å¯ä»¥æŸ¥è¯¢ï¼Œä¸ºä»€ä¹ˆè¿˜è¦å»ºç«‹ hierarchy çš„ shadow page table ?
- [x] hash page table ä¸­é—´æ”¾ç½®æ‰€æœ‰çš„ä» gva åˆ° hpa çš„åœ°å€ ?

- å»ºç«‹ hash æ˜¯ä¸ºäº†è®© guest çš„ page table å’Œ host çš„ shadow page table ä¹‹é—´å¯ä»¥å¿«é€ŸæŸ¥æ‰¾.
- shadow page table : gva åˆ° hpa çš„æ˜ å°„ï¼Œè¿™ä¸ªæ˜ å°„æ˜¯ä¸€ä¸ª tree çš„ç»“æ„


## sync shadow page
1. åˆ©ç”¨ generation æ¥å®ç°å®šä½ ?

```c
static bool is_obsolete_sp(struct kvm *kvm, struct kvm_mmu_page *sp)
{
    return sp->role.invalid ||
           unlikely(sp->mmu_valid_gen != kvm->arch.mmu_valid_gen);
}
```


## paging_tmpl.h

We need the mmu code to access both 32-bit and 64-bit guest ptes,
so the code in this file is compiled twice, once per pte size.

- [x] å¦‚ä½•å®ç°å¤šæ¬¡ç¼–è¯‘ ? ç›®çš„åº”è¯¥æ˜¯æä¾›ä¸‰ç§ä¸åŒç¼–è¯‘å±æ€§çš„æ–‡ä»¶ï¼Œå…¶ä¸­åªæ˜¯å°‘é‡åç§»é‡çš„ä¿®æ”¹ã€‚é€šè¿‡ä¸‰æ¬¡ include è§£å†³.
- [ ] å¦‚æœ guest ä½¿ç”¨ transparent huge page çš„æ—¶å€™ï¼Œå…¶æä¾›çš„ page walk æ€ä¹ˆåŠ ?


```c
static void shadow_mmu_init_context(struct kvm_vcpu *vcpu, struct kvm_mmu *context,
                    u32 cr0, u32 cr4, u32 efer,
                    union kvm_mmu_role new_role)
{
    if (!(cr0 & X86_CR0_PG))
        nonpaging_init_context(vcpu, context);
    else if (efer & EFER_LMA)
        paging64_init_context(vcpu, context);
    else if (cr4 & X86_CR4_PAE)
        paging32E_init_context(vcpu, context);
    else
        paging32_init_context(vcpu, context);

    context->mmu_role.as_u64 = new_role.as_u64;
    reset_shadow_zero_bits_mask(vcpu, context);
}
```
> éƒ½æ˜¯æä¾›çš„ shadow çš„æƒ…å†µï¼Œé‚£ä¹ˆ ept å’Œ tdp æ‰€ä»¥æ²¡æœ‰å‡ºç° ?

## shadow page table
- [ ] shadow page table æ˜¯æ”¾åœ¨ qemu çš„ç©ºé—´ä¸­é—´ï¼Œè¿˜æ˜¯å†…æ ¸åœ°å€ç©ºé—´
  - guest é€šè¿‡ cr3 å¯ä»¥æ¥è®¿é—®
  - å†…æ ¸å¯ä»¥æ“æ§ page table
- [ ] guest çš„å†…æ ¸ vmalloc ä¿®æ”¹ page tableï¼Œæ˜¯é¦–å…ˆä¿®æ”¹ shadow page table é€ æˆçš„å¼‚å¸¸ï¼Œç„¶åä¹‹åæ‰ä¿®æ”¹ guest page table ?
    - [ ] shadow page table å„ä¸ªçº§åˆ«å­˜æ”¾çš„åœ°å€æ˜¯ä»€ä¹ˆ ? ç‰©ç†åœ°å€ï¼Œå› ä¸ºæ˜¯è®© cr3 ä½¿ç”¨çš„
    - [x] guest page table çš„å†…å®¹ ? GVA ä¹Ÿå°±æ˜¯ host çš„è™šæ‹Ÿåœ°å€
- [x] `FNAME(walk_addr)()` å­˜å‚¨çš„åœ°å€éƒ½æ˜¯ guest çš„è™šæ‹Ÿåœ°å€ ? æ˜¯çš„ï¼Œæ‰€ä»¥åº”è¯¥å¾ˆå®¹æ˜“ walk.

> FNAME(walk_addr)() æŸ¥ guesté¡µè¡¨ï¼Œç‰©ç†åœ°å€æ˜¯å¦å­˜åœ¨ï¼Œè¿™æ—¶è‚¯å®šæ˜¯ä¸å­˜åœ¨çš„
`inject_page_fault()->kvm_inject_page_fault()` å¼‚å¸¸æ³¨å…¥æµç¨‹ï¼›

åœ¨ Host ä¸­é—´æ£€æŸ¥å‘ç°ä¸å­˜åœ¨ï¼Œç„¶ååœ¨ä½¿ç”¨ inject pg åˆ° guest.
å› ä¸º guest page table å­˜åœ¨å¤šä¸ªæ¨¡å‹

è®© Host è¶Šä¿ä»£åº–æ¥èµ°ä¸€é guest çš„ page walkï¼Œshadow page table æ˜¯ CR3 ä¸­é—´å®é™…ä½¿ç”¨çš„ page table.
-> ä½¿ç”¨ spt ï¼Œå‡ºç° exception æ˜¯ä¸çŸ¥é“åˆ°åº•å“ªä¸€ä¸ªå±‚æ¬¡å‡ºç°é—®é¢˜çš„, æ‰€ä»¥éƒ½æ˜¯éœ€è¦æŠ›å‡ºæ¥æ£€æŸ¥çš„
-> *é‚£ä¹ˆå½“ guest é€šè¿‡ cr3 è¿›è¡Œä¿®æ”¹ shadow page table çš„æ—¶å€™ï¼Œé€šè¿‡ write protection å¯ä»¥æ‰¾åˆ° ?*
-> *å¥½åƒ shadow page åªèƒ½å­˜æ”¾ 512 ä¸ª page table entry,  åˆ©ç”¨ cr3 è®¿é—®çœŸçš„æ²¡æœ‰é—®é¢˜å— ?*

> å½±å­é¡µè¡¨åˆæ˜¯è½½å…¥åˆ°CR3ä¸­çœŸæ­£ä¸ºç‰©ç†MMUæ‰€åˆ©ç”¨è¿›è¡Œå¯»å€çš„é¡µè¡¨ï¼Œå› æ­¤å¼€å§‹æ—¶ä»»ä½•çš„å†…å­˜è®¿é—®æ“ä½œéƒ½ä¼šå¼•èµ·ç¼ºé¡µå¼‚å¸¸ï¼›å¯¼è‡´vmå‘ç”ŸVM Exitï¼›è¿›å…¥handle_exception();

## æ‰¾åˆ° shadow ä»¥åŠ ept çš„ page table entry


## rmap
https://www.cnblogs.com/ck1020/p/6920765.html

åœ¨KVMä¸­ï¼Œé€†å‘æ˜ å°„æœºåˆ¶çš„ä½œç”¨æ˜¯ç±»ä¼¼çš„ï¼Œä½†æ˜¯å®Œæˆçš„å´ä¸æ˜¯ä»HPAåˆ°å¯¹åº”çš„EPTé¡µè¡¨é¡¹çš„å®šä½ï¼Œ
è€Œæ˜¯ä»gfnåˆ°*å¯¹åº”çš„é¡µè¡¨é¡¹*çš„å®šä½ã€‚
*ç†è®ºä¸Šè®²æ ¹æ®gfnä¸€æ­¥æ­¥éå†EPTä¹Ÿæœªå°ä¸å¯ï¼Œä½†æ˜¯æ•ˆç‡è¾ƒä½*å†µä¸”åœ¨EPTæ‰€ç»´æŠ¤çš„é¡µé¢ä¸åŒäºhostçš„é¡µè¡¨ï¼Œ*ç†è®ºä¸Šè®²æ˜¯è™šæ‹Ÿæœºä¹‹é—´æ˜¯ç¦æ­¢ä¸»åŠ¨çš„å…±äº«å†…å­˜çš„*ï¼Œä¸ºäº†æé«˜æ•ˆç‡ï¼Œå°±æœ‰äº†å½“å‰çš„é€†å‘æ˜ å°„æœºåˆ¶ã€‚

- rmap: from guest page to shadow ptes that map it
- Shadow hash: from guest page to its shadow
- Parent pte chain: from shaow page to upperlevel shadow page
- Shadow pte: from shadow page to lower-level shadow page
- LRU: all active shadow pages

Walk the shadow page table, instantiating page tables as necessary
- Can involve an rmap walk and *write protecting the guest page table*


```c
struct kvm_arch_memory_slot {
  // åº”è¯¥æ˜¯ä¸€ç§ page size ç„¶åæä¾›ä¸€ç§ rmap å§
    struct kvm_rmap_head *rmap[KVM_NR_PAGE_SIZES];
    struct kvm_lpage_info *lpage_info[KVM_NR_PAGE_SIZES - 1];
    unsigned short *gfn_track[KVM_PAGE_TRACK_MAX];
};

#define KVM_MAX_HUGEPAGE_LEVEL  PG_LEVEL_1G
#define KVM_NR_PAGE_SIZES   (KVM_MAX_HUGEPAGE_LEVEL - PG_LEVEL_4K + 1)

enum pg_level {
    PG_LEVEL_NONE,
    PG_LEVEL_4K,
    PG_LEVEL_2M,
    PG_LEVEL_1G,
    PG_LEVEL_512G,
    PG_LEVEL_NUM
};
```

```c
static int kvm_alloc_memslot_metadata(struct kvm_memory_slot *slot,
                      unsigned long npages)
    // æ¯ä¸€ä¸ª page éƒ½ä¼šå»ºç«‹ä¸€ä¸ª
        slot->arch.rmap[i] =
            kvcalloc(lpages, sizeof(*slot->arch.rmap[i]),
    // ....
}

// mmu_set_spte çš„åœ°æ–¹è°ƒç”¨
static int rmap_add(struct kvm_vcpu *vcpu, u64 *spte, gfn_t gfn)
{
    struct kvm_mmu_page *sp;
    struct kvm_rmap_head *rmap_head;

  // é€šè¿‡ pte çš„æŒ‡é’ˆï¼Œè·å– spte æŒ‡å‘çš„ pte æ‰€åœ¨çš„ page çš„
    sp = sptep_to_sp(spte);
  // shadow å’Œ direct éƒ½æ˜¯éœ€è¦ rmap
  // ä½†æ˜¯ï¼Œdirect å…¶å®å¹¶ä¸ä¼šæ³¨å†Œ
    kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn);
    rmap_head = gfn_to_rmap(vcpu->kvm, gfn, sp);
    return pte_list_add(vcpu, spte, rmap_head);
}
```

```c
static gfn_t kvm_mmu_page_get_gfn(struct kvm_mmu_page *sp, int index)
{
    if (!sp->role.direct)
        return sp->gfns[index];

  // TODO guest çš„ç‰©ç†é¡µé¢åº”è¯¥å°±æ˜¯è¿ç»­çš„å•Š!
  // å½“ level åœ¨æœ€åº•å±‚çš„æ—¶å€™ï¼Œsp->gfn + index å°±å¯ä»¥äº†å•Š!
    return sp->gfn + (index << ((sp->role.level - 1) * PT64_LEVEL_BITS));
}


static struct kvm_rmap_head *gfn_to_rmap(struct kvm *kvm, gfn_t gfn,
                     struct kvm_mmu_page *sp)
{
    struct kvm_memslots *slots;
    struct kvm_memory_slot *slot;

    slots = kvm_memslots_for_spte_role(kvm, sp->role);
    slot = __gfn_to_memslot(slots, gfn);
    return __gfn_to_rmap(gfn, sp->role.level, slot);
}
```


- [ ] å»ºç«‹åå‘æ˜ å°„çš„åŸå› æ˜¯ : å½“ shadow page table è¿›è¡Œä¿®æ”¹ä¹‹åï¼Œéœ€è¦çŸ¥é“å…¶æ‰€åœ¨çš„ gfn
  - [ ] çœŸçš„å­˜åœ¨æ ¹æ® shadow page table åˆ° gfn çš„éœ€æ±‚å— ?
- [ ] direct éœ€è¦ rmap å— ? æ˜¾ç„¶éœ€è¦ï¼Œä¸ç„¶ direct_map ä¸ä¼šè°ƒç”¨ rmap_add


```c
    kvm_mmu_page_set_gfn(sp, spte - sp->spt, gfn); // ä¸€ä¸ª shadow page å’Œ gfn çš„å…³ç³»
    rmap_head = gfn_to_rmap(vcpu->kvm, gfn, sp);
    return pte_list_add(vcpu, spte, rmap_head); // slot çš„æ¯ä¸€ä¸ª page éƒ½ä¼šè¢« rmap
```

å®é™…ä¸Šï¼Œå­˜åœ¨ä¸¤ä¸ª rmap
- `sp->gfns` è·å–æ¯ä¸€ä¸ª pte å¯¹åº”çš„ gfn
- `rmap_head->val` = spte : è¿™ä¸æ˜¯ rmap å§

#### parent rmap
```c
static void mmu_page_add_parent_pte(struct kvm_vcpu *vcpu,
                    struct kvm_mmu_page *sp, u64 *parent_pte)
{
    if (!parent_pte)
        return;

    pte_list_add(vcpu, parent_pte, &sp->parent_ptes);
}
```

#### rmap iterator
- [x] rmap æ€»æ˜¯æ„å»ºçš„ rmap_head åˆ° sptep å— ?
  - rmap_add å’Œ mmu_page_add_parent_pte éƒ½æ˜¯çš„

è§£æ for_each_rmap_spte
```c
#define for_each_rmap_spte(_rmap_head_, _iter_, _spte_)         \
    for (_spte_ = rmap_get_first(_rmap_head_, _iter_);      \
         _spte_; _spte_ = rmap_get_next(_iter_))
```
ä½¿ç”¨ä½ç½®:
kvm_mmu_write_protect_pt_masked : ç»™å®š gfn_offsetï¼Œå°†å…³è”çš„æ‰€æœ‰çš„ spte å…¨éƒ¨æ·»åŠ  flags

kvm_set_pte_rmapp : å°† rmap_head çš„æŒæœ‰çš„æ‰€æœ‰çš„ sptep è¿›è¡Œè®¾ç½®

#### gfn_to_memslot_dirty_bitmap
`slot->dirty_bitmap` éƒ½åœ¨ kvm_main ä¸Šé¢è®¿é—®

pte_prefetch_gfn_to_pfn

- [ ] dirty æŒ‡çš„æ˜¯ è° ç›¸å¯¹äº è° æ˜¯ dirty çš„

- æœ€åè¢« `__direct_map` è°ƒç”¨


## ğŸ‘‡è®°å½• mmu.rst çš„å†…å®¹:
è™½ç„¶çš„ç¡®è§£é‡Šäº† mmio ä½¿ç”¨ generation çš„åŸå› ï¼Œä½†æ˜¯ä¸‹é¢çš„é—®é¢˜å€¼å¾—ç†è§£:
- [ ] As mentioned in "Reaction to events" above, kvm will cache MMIO information in leaf sptes.
  - [ ] å¦‚æœä¸ cache, è¿™äº›æ•°æ®æ”¾åœ¨é‚£é‡Œ

- [ ] When a new memslot is added or an existing memslot is changed, this information may become stale and needs to be invalidated.
  - [ ] ä¸ºä»€ä¹ˆ memslot å¢åŠ ï¼Œå¯¼è‡´æ•°æ®å¤±æ•ˆ

Unfortunately, a single memory access might access kvm_memslots(kvm) multiple
times, the last one happening when the generation number is retrieved and
stored into the MMIO spte.  Thus, the MMIO spte might be created based on
out-of-date information, but with an up-to-date generation number.

- [ ] To avoid this, the generation number is incremented again after synchronize_srcu
returns;

- [ ] æ‰¾åˆ°è®¿é—® pte æ¥æ¯”è¾ƒ generation, å‘ç° out of dateï¼Œç„¶å slow path çš„ä»£ç 

## TODO : shadow flood
